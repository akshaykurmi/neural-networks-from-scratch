{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./load_nnfs_module.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import IrisFlowers\n",
    "from nnfs.activations import ReLU, Softmax\n",
    "from nnfs.losses import CategoricalCrossentropy\n",
    "from nnfs.optimizers import Momentum\n",
    "from nnfs.initializers import RandomUniform, Zeros\n",
    "from nnfs.metrics import CategoricalAccuracy\n",
    "from nnfs.layers import Dense, Dropout\n",
    "from nnfs.models import FeedForwardNetwork\n",
    "from nnfs.utils.data import split_data\n",
    "from nnfs.utils.preprocessing import OneHotEncoder, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has already been downloaded.\n"
     ]
    }
   ],
   "source": [
    "dataset = IrisFlowers()\n",
    "dataset.download_data()\n",
    "X, y = dataset.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(y)\n",
    "X = X.to_numpy()\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, ratio=0.8)\n",
    "X_train, X_test = normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForwardNetwork()\n",
    "model.add(Dense(units=64,\n",
    "                activation=ReLU(),\n",
    "                weights_initializer=RandomUniform(),\n",
    "                bias_initializer=Zeros(),\n",
    "                input_shape=(BATCH_SIZE, 4)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=3,\n",
    "                activation=Softmax(),\n",
    "                weights_initializer=RandomUniform(),\n",
    "                bias_initializer=Zeros()))\n",
    "model.compile(optimizer=Momentum(learning_rate=0.001, nesterov=True, clip_value=0.05),\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=[CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1000/1000 |██████████| ETA: 00:00 |365.70epoch/s, CategoricalCrossentropy=0.0648, CategoricalAccuracy=0.975"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.012335071143920242\n",
      "Test Metrics : {'CategoricalAccuracy': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=1000)\n",
    "loss, metrics = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(\"Test Loss :\", loss)\n",
    "print(\"Test Metrics :\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
